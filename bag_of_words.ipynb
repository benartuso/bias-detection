{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_data/WNC/biased.word.dev',\n",
       " 'bias_data/WNC/revision_topics.csv',\n",
       " 'bias_data/WNC/biased.word.test',\n",
       " 'bias_data/WNC/neutral',\n",
       " 'bias_data/WNC/biased.word.train',\n",
       " 'bias_data/WNC/biased.full']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('bias_data/WNC/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "\n",
    "Let's start out with the train / dev subsets proposed by the authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('bias_data/WNC/biased.word.train', sep='\\t', header=None)\n",
    "dev = pd.read_csv('bias_data/WNC/biased.word.dev', sep='\\t', header=None)\n",
    "test = pd.read_csv('bias_data/WNC/biased.word.test', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 53803\n",
      "Dev length: 700\n",
      "Test length: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train length: {}\".format(len(train)))\n",
    "print(\"Dev length: {}\".format(len(dev)))\n",
    "print(\"Test length: {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_data(train_path, dev_path):\n",
    "    train = pd.read_csv(train_path, sep='\\t', header=None)\n",
    "    dev = pd.read_csv(dev_path, sep = '\\t', header=None)\n",
    "    \n",
    "    print(\"Train length: {}\".format(len(train)))\n",
    "    print(\"Dev length: {}\".format(len(dev)))\n",
    "    \n",
    "    #Rename columns\n",
    "    train.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "    dev.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "    \n",
    "    #Process and recombine training data: \n",
    "    train_biased, dev_biased = pd.DataFrame(train.biased), pd.DataFrame(dev.biased)\n",
    "    train_unbiased, dev_unbiased = pd.DataFrame(train.unbiased), pd.DataFrame(dev.unbiased)\n",
    "    \n",
    "    \n",
    "    train_biased['label'], dev_biased['label'] = [1]*len(train_biased), [1]*len(dev_biased)\n",
    "    train_unbiased['label'], dev_unbiased['label'] = [0]*len(train_unbiased), [0]*len(dev_unbiased)\n",
    "\n",
    "    #Fix colnames\n",
    "    train_biased.columns, dev_biased.columns = ['text', 'label'], ['text', 'label']\n",
    "    train_unbiased.columns, dev_unbiased.columns = ['text', 'label'], ['text', 'label']\n",
    "    #Combine\n",
    "    train_all, dev_all = pd.concat([train_biased, train_unbiased]), pd.concat([dev_biased, dev_unbiased])\n",
    "    \n",
    "    return train_all, dev_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 53803\n",
      "Dev length: 700\n"
     ]
    }
   ],
   "source": [
    "train, dev = read_process_data('bias_data/WNC/biased.word.train', 'bias_data/WNC/biased.word.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "dev.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_biased = pd.DataFrame(train.biased)\n",
    "train_unbiased = pd.DataFrame(train.unbiased)\n",
    "\n",
    "#Add labels\n",
    "train_biased['label'] = [1]*len(train_biased)\n",
    "train_unbiased['label'] = [0]*len(train_unbiased)\n",
    "\n",
    "#Fix colnames\n",
    "train_biased.columns = ['text', 'label']\n",
    "train_unbiased.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shmush\n",
    "train_all = pd.concat([train_biased, train_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in addition to sponsoring palestinian terror a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the game is currently played in 47 countries w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no part of the valley lies in the area current...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scholars perceived that it was discordant with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>since the chinese civil war in 1949, taiwan ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>in 2008 five pharmaceutical companies received...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>the palm, a steakhouse restaurant chain origin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>d.c. united's early successes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>on 29 june 2007 price gave birth to her third ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>on 7 april, reuters reported that soldiers loy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    in addition to sponsoring palestinian terror a...      1\n",
       "1    the game is currently played in 47 countries w...      1\n",
       "2    no part of the valley lies in the area current...      1\n",
       "3    scholars perceived that it was discordant with...      1\n",
       "4    since the chinese civil war in 1949, taiwan ha...      1\n",
       "..                                                 ...    ...\n",
       "695  in 2008 five pharmaceutical companies received...      0\n",
       "696  the palm, a steakhouse restaurant chain origin...      0\n",
       "697                      d.c. united's early successes      0\n",
       "698  on 29 june 2007 price gave birth to her third ...      0\n",
       "699  on 7 april, reuters reported that soldiers loy...      0\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_biased = pd.DataFrame(dev.biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct bag-of-words representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize vectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = vectorizer.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107606, 79211)\n",
      "(1400, 79211)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1A: Plain bag of words into logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Was running out of iterations hahaa\n",
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.77603479359887\n",
      "Val set accuracy: 0.6814285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1B: Plain bag of words into random forest\n",
    "Notice the SEVERE overfitting here, which is strange given the model used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9942103600170994\n",
      "Val set accuracy: 0.6757142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(rf.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Tfidf()\n",
    "X_train = tfidf.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = tfidf.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2a: TF-IDF Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "#This one finishes in way fewer iterations! Interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.706196680482501\n",
      "Val set accuracy: 0.6842857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2b: TF-IDF Random Forest\n",
    "Still overfitting, which makes sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9942475326654647\n",
      "Val set accuracy: 0.6564285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(rf.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    700\n",
       "0    700\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_dev).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zeugma\n",
      "  Downloading zeugma-0.48.tar.gz (10 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (1.18.1)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (0.29.15)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (1.0.3)\n",
      "Collecting gensim>=3.5.0\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit_learn>=0.19.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (0.22.1)\n",
      "Collecting tensorflow>=1.5.0\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.1 MB 40 kB/s  eta 0:00:013\n",
      "\u001b[?25hCollecting keras>=2.1.3\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ben/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.3->zeugma) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.3->zeugma) (2.8.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from gensim>=3.5.0->zeugma) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from gensim>=3.5.0->zeugma) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ben/anaconda3/lib/python3.7/site-packages (from scikit_learn>=0.19.1->zeugma) (0.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (1.11.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.34.0-cp37-cp37m-macosx_10_10_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (0.34.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/ben/anaconda3/lib/python3.7/site-packages (from keras>=2.1.3->zeugma) (5.3)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.22.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (45.2.0.post20200210)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.0.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 4.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 6.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.24.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/ben/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.5.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ben/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.2.0)\n",
      "Building wheels for collected packages: zeugma, smart-open, termcolor\n",
      "  Building wheel for zeugma (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zeugma: filename=zeugma-0.48-py3-none-any.whl size=8774 sha256=f90ed4ab898b389053b85810b37ebf897fd1861b83cfbd5c51d525b5fc1ab37a\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/36/66/76/e70dc6a27742b760eef56eff41e6a7db1235990a9ac1d9d2d0\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108249 sha256=8de0c8b1a0b6b0b64c5b6652eff9b08171cfc9fbcdb7649b3123ac3807c0cd76\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/34/3d/14/f19c01a19c9201cdb6a76b049904d5226912569be919ad1eae\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=32f60397e9de48deecbc708f02e54efc6936a07a3fea216c6433074e3122f03d\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built zeugma smart-open termcolor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: smart-open, gensim, tensorflow-estimator, grpcio, google-pasta, opt-einsum, termcolor, protobuf, keras-preprocessing, gast, astunparse, absl-py, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, google-auth-oauthlib, markdown, tensorboard, tensorflow, keras, zeugma\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 gensim-3.8.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.34.0 keras-2.4.3 keras-preprocessing-1.1.2 markdown-3.3.3 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.6 smart-open-4.0.1 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 zeugma-0.48\n"
     ]
    }
   ],
   "source": [
    "!pip install zeugma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: Using embeddings?\n",
    "\n",
    "Plain rf on the data was taking way too long, imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "from sklearn.\n",
    "glove = EmbeddingTransformer('glove')\n",
    "X_train = glove.transform(train.text)\n",
    "y_train = train.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = glove.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5457142857142857"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5556381614408118\n",
      "0.5557142857142857\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train,y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well, this is all quite a bummer, isn't it? \n",
    "Looks like tf-idf with logistic hecking regression is the winner. Let's tune 'er up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.706196680482501\n",
      "Val set accuracy: 0.6842857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "\n",
    "tfidf = Tfidf()\n",
    "X_train = tfidf.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = tfidf.transform(dev.text)\n",
    "y_dev = dev.label\n",
    "\n",
    "### Attempt 2a: TF-IDF Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "#This one finishes in way fewer iterations! Interesting\n",
    "\n",
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll come back to that. \n",
    "\n",
    "Let's try doing the bigger set...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_data/WNC/biased.word.dev',\n",
       " 'bias_data/WNC/revision_topics.csv',\n",
       " 'bias_data/WNC/biased.word.test',\n",
       " 'bias_data/WNC/neutral',\n",
       " 'bias_data/WNC/biased.word.train',\n",
       " 'bias_data/WNC/biased.full']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('bias_data/WNC/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 60908: expected 7 fields, saw 9\\n'\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv('bias_data/WNC/biased.full', sep='\\t', header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dennis the menace is an american animated series produced by dic entertainment, based on the comic strip by hank ketcham.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()[4][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_biased = pd.DataFrame(full[3])\n",
    "full_unbiased = pd.DataFrame(full[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_biased['label'] = [1]*len(full_biased)\n",
    "full_unbiased['label'] = [0]*len(full_unbiased)\n",
    "\n",
    "\n",
    "full_biased.columns = ['text', 'label']\n",
    "full_unbiased.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([full_biased, full_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>during the campaign, controversy erupted over ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nicaea was convoked by the emperor constantine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it was rather unfortunate that he vehemently o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dennis the menace is an american animated seri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>today, on large farms, motorcycles, dogs or me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  during the campaign, controversy erupted over ...      1\n",
       "1  nicaea was convoked by the emperor constantine...      1\n",
       "2  it was rather unfortunate that he vehemently o...      1\n",
       "3  dennis the menace is an american animated seri...      1\n",
       "4  today, on large farms, motorcycles, dogs or me...      1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181468</th>\n",
       "      <td>arguably the most notable was dale earnhardt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181469</th>\n",
       "      <td>because of the genetic prepotency of the ancie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181470</th>\n",
       "      <td>influenced by serge gainsbourg, the velvet und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181471</th>\n",
       "      <td>northern indiana, for example, contains the in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181472</th>\n",
       "      <td>mythology is alive and well in the modern age ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "181468  arguably the most notable was dale earnhardt, ...      0\n",
       "181469  because of the genetic prepotency of the ancie...      0\n",
       "181470  influenced by serge gainsbourg, the velvet und...      0\n",
       "181471  northern indiana, for example, contains the in...      0\n",
       "181472  mythology is alive and well in the modern age ...      0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.sample(random_state=42, frac=1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130694</th>\n",
       "      <td>on 24 january 1953 mau mau, possibly former se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160318</th>\n",
       "      <td>support for windows media drm (wmdrm) (incompa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98343</th>\n",
       "      <td>when the kingdom became independent, it was co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35321</th>\n",
       "      <td>in may 2007 farley mowat was claimed to be hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136178</th>\n",
       "      <td>world war ii left the united kingdom with an a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143303</th>\n",
       "      <td>fifteen of the sixteen german states, positing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30326</th>\n",
       "      <td>the powerbook 500 series was the mainstay of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58750</th>\n",
       "      <td>this model has been rejected by the scientific...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75834</th>\n",
       "      <td>on 29 august 2010, bogdan made his premier lea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153634</th>\n",
       "      <td>two nations emerged in what is now argentina:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "130694  on 24 january 1953 mau mau, possibly former se...      0\n",
       "160318  support for windows media drm (wmdrm) (incompa...      0\n",
       "98343   when the kingdom became independent, it was co...      0\n",
       "35321   in may 2007 farley mowat was claimed to be hea...      0\n",
       "136178  world war ii left the united kingdom with an a...      0\n",
       "...                                                   ...    ...\n",
       "143303  fifteen of the sixteen german states, positing...      1\n",
       "30326   the powerbook 500 series was the mainstay of t...      0\n",
       "58750   this model has been rejected by the scientific...      0\n",
       "75834   on 29 august 2010, bogdan made his premier lea...      1\n",
       "153634      two nations emerged in what is now argentina:      1\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = full.iloc[0:308504]\n",
    "full_dev = full.iloc[308504:326651]\n",
    "full_test = full.iloc[326651:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326651.4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full) * 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train.to_csv('processed_data/full_train.csv', index=False, header=False)\n",
    "full_dev.to_csv('processed_data/full_dev.csv', index=False, header=False)\n",
    "full_test.to_csv('processed_data/full_test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
