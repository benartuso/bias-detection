{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_data/WNC/biased.word.dev',\n",
       " 'bias_data/WNC/revision_topics.csv',\n",
       " 'bias_data/WNC/biased.word.test',\n",
       " 'bias_data/WNC/neutral',\n",
       " 'bias_data/WNC/biased.word.train',\n",
       " 'bias_data/WNC/biased.full']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('bias_data/WNC/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "\n",
    "Let's start out with the train / dev subsets proposed by the authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('bias_data/WNC/biased.word.train', sep='\\t', header=None)\n",
    "dev = pd.read_csv('bias_data/WNC/biased.word.dev', sep='\\t', header=None)\n",
    "test = pd.read_csv('bias_data/WNC/biased.word.test', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 53803\n",
      "Dev length: 700\n",
      "Test length: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train length: {}\".format(len(train)))\n",
    "print(\"Dev length: {}\".format(len(dev)))\n",
    "print(\"Test length: {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_data(train_path, dev_path):\n",
    "    train = pd.read_csv(train_path, sep='\\t', header=None)\n",
    "    dev = pd.read_csv(dev_path, sep = '\\t', header=None)\n",
    "    \n",
    "    print(\"Train length: {}\".format(len(train)))\n",
    "    print(\"Dev length: {}\".format(len(dev)))\n",
    "    \n",
    "    #Rename columns\n",
    "    train.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "    dev.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "    \n",
    "    #Process and recombine training data: \n",
    "    train_biased, dev_biased = pd.DataFrame(train.biased), pd.DataFrame(dev.biased)\n",
    "    train_unbiased, dev_unbiased = pd.DataFrame(train.unbiased), pd.DataFrame(dev.unbiased)\n",
    "    \n",
    "    \n",
    "    train_biased['label'], dev_biased['label'] = [1]*len(train_biased), [1]*len(dev_biased)\n",
    "    train_unbiased['label'], dev_unbiased['label'] = [0]*len(train_unbiased), [0]*len(dev_unbiased)\n",
    "\n",
    "    #Fix colnames\n",
    "    train_biased.columns, dev_biased.columns = ['text', 'label'], ['text', 'label']\n",
    "    train_unbiased.columns, dev_unbiased.columns = ['text', 'label'], ['text', 'label']\n",
    "    #Combine\n",
    "    train_all, dev_all = pd.concat([train_biased, train_unbiased]), pd.concat([dev_biased, dev_unbiased])\n",
    "    \n",
    "    return train_all, dev_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 53803\n",
      "Dev length: 700\n"
     ]
    }
   ],
   "source": [
    "train, dev = read_process_data('bias_data/WNC/biased.word.train', 'bias_data/WNC/biased.word.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']\n",
    "dev.columns = ['id', 'annot_old', 'annot_new', 'biased', 'unbiased', 'tags', 'roots']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_biased = pd.DataFrame(train.biased)\n",
    "train_unbiased = pd.DataFrame(train.unbiased)\n",
    "\n",
    "#Add labels\n",
    "train_biased['label'] = [1]*len(train_biased)\n",
    "train_unbiased['label'] = [0]*len(train_unbiased)\n",
    "\n",
    "#Fix colnames\n",
    "train_biased.columns = ['text', 'label']\n",
    "train_unbiased.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shmush\n",
    "train_all = pd.concat([train_biased, train_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in addition to sponsoring palestinian terror a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the game is currently played in 47 countries w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no part of the valley lies in the area current...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scholars perceived that it was discordant with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>since the chinese civil war in 1949, taiwan ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>in 2008 five pharmaceutical companies received...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>the palm, a steakhouse restaurant chain origin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>d.c. united's early successes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>on 29 june 2007 price gave birth to her third ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>on 7 april, reuters reported that soldiers loy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    in addition to sponsoring palestinian terror a...      1\n",
       "1    the game is currently played in 47 countries w...      1\n",
       "2    no part of the valley lies in the area current...      1\n",
       "3    scholars perceived that it was discordant with...      1\n",
       "4    since the chinese civil war in 1949, taiwan ha...      1\n",
       "..                                                 ...    ...\n",
       "695  in 2008 five pharmaceutical companies received...      0\n",
       "696  the palm, a steakhouse restaurant chain origin...      0\n",
       "697                      d.c. united's early successes      0\n",
       "698  on 29 june 2007 price gave birth to her third ...      0\n",
       "699  on 7 april, reuters reported that soldiers loy...      0\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_biased = pd.DataFrame(dev.biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct bag-of-words representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize vectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = vectorizer.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107606, 79211)\n",
      "(1400, 79211)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1A: Plain bag of words into logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Was running out of iterations hahaa\n",
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.77603479359887\n",
      "Val set accuracy: 0.6814285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1B: Plain bag of words into random forest\n",
    "Notice the SEVERE overfitting here, which is strange given the model used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9942103600170994\n",
      "Val set accuracy: 0.6757142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(rf.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Tfidf()\n",
    "X_train = tfidf.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = tfidf.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2a: TF-IDF Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "#This one finishes in way fewer iterations! Interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.706196680482501\n",
      "Val set accuracy: 0.6842857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2b: TF-IDF Random Forest\n",
    "Still overfitting, which makes sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9942475326654647\n",
      "Val set accuracy: 0.6564285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy: {}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(rf.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    700\n",
       "0    700\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_dev).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zeugma\n",
      "  Downloading zeugma-0.48.tar.gz (10 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (1.18.1)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (0.29.15)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (1.0.3)\n",
      "Collecting gensim>=3.5.0\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit_learn>=0.19.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from zeugma) (0.22.1)\n",
      "Collecting tensorflow>=1.5.0\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165.1 MB 40 kB/s  eta 0:00:013\n",
      "\u001b[?25hCollecting keras>=2.1.3\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ben/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.3->zeugma) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from pandas>=0.20.3->zeugma) (2.8.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117 kB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from gensim>=3.5.0->zeugma) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from gensim>=3.5.0->zeugma) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ben/anaconda3/lib/python3.7/site-packages (from scikit_learn>=0.19.1->zeugma) (0.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (1.11.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 459 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.34.0-cp37-cp37m-macosx_10_10_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.6 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorflow>=1.5.0->zeugma) (0.34.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.6 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/ben/anaconda3/lib/python3.7/site-packages (from keras>=2.1.3->zeugma) (5.3)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.22.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (45.2.0.post20200210)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ben/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.0.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96 kB 4.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47 kB 6.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.24.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/ben/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (1.5.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /Users/ben/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ben/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.5.0->zeugma) (2.2.0)\n",
      "Building wheels for collected packages: zeugma, smart-open, termcolor\n",
      "  Building wheel for zeugma (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zeugma: filename=zeugma-0.48-py3-none-any.whl size=8774 sha256=f90ed4ab898b389053b85810b37ebf897fd1861b83cfbd5c51d525b5fc1ab37a\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/36/66/76/e70dc6a27742b760eef56eff41e6a7db1235990a9ac1d9d2d0\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108249 sha256=8de0c8b1a0b6b0b64c5b6652eff9b08171cfc9fbcdb7649b3123ac3807c0cd76\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/34/3d/14/f19c01a19c9201cdb6a76b049904d5226912569be919ad1eae\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=32f60397e9de48deecbc708f02e54efc6936a07a3fea216c6433074e3122f03d\n",
      "  Stored in directory: /Users/ben/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built zeugma smart-open termcolor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: smart-open, gensim, tensorflow-estimator, grpcio, google-pasta, opt-einsum, termcolor, protobuf, keras-preprocessing, gast, astunparse, absl-py, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, google-auth-oauthlib, markdown, tensorboard, tensorflow, keras, zeugma\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 gensim-3.8.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.34.0 keras-2.4.3 keras-preprocessing-1.1.2 markdown-3.3.3 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.6 smart-open-4.0.1 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0 zeugma-0.48\n"
     ]
    }
   ],
   "source": [
    "!pip install zeugma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: Using embeddings?\n",
    "\n",
    "Plain rf on the data was taking way too long, imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "from sklearn.\n",
    "glove = EmbeddingTransformer('glove')\n",
    "X_train = glove.transform(train.text)\n",
    "y_train = train.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = glove.transform(dev.text)\n",
    "y_dev = dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5457142857142857"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5556381614408118\n",
      "0.5557142857142857\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train,y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well, this is all quite a bummer, isn't it? \n",
    "Looks like tf-idf with logistic hecking regression is the winner. Let's tune 'er up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.706196680482501\n",
      "Val set accuracy: 0.6842857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "\n",
    "tfidf = Tfidf()\n",
    "X_train = tfidf.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = tfidf.transform(dev.text)\n",
    "y_dev = dev.label\n",
    "\n",
    "### Attempt 2a: TF-IDF Logistic Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "#This one finishes in way fewer iterations! Interesting\n",
    "\n",
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll come back to that. \n",
    "\n",
    "Let's try doing the bigger set...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_data/WNC/biased.word.dev',\n",
       " 'bias_data/WNC/revision_topics.csv',\n",
       " 'bias_data/WNC/biased.word.test',\n",
       " 'bias_data/WNC/neutral',\n",
       " 'bias_data/WNC/biased.word.train',\n",
       " 'bias_data/WNC/biased.full']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('bias_data/WNC/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 60908: expected 7 fields, saw 9\\n'\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv('bias_data/WNC/biased.full', sep='\\t', header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dennis the menace is an american animated series produced by dic entertainment, based on the comic strip by hank ketcham.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()[4][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_biased = pd.DataFrame(full[3])\n",
    "full_unbiased = pd.DataFrame(full[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_biased['label'] = [1]*len(full_biased)\n",
    "full_unbiased['label'] = [0]*len(full_unbiased)\n",
    "\n",
    "\n",
    "full_biased.columns = ['text', 'label']\n",
    "full_unbiased.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([full_biased, full_unbiased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>during the campaign, controversy erupted over ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nicaea was convoked by the emperor constantine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it was rather unfortunate that he vehemently o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dennis the menace is an american animated seri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>today, on large farms, motorcycles, dogs or me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  during the campaign, controversy erupted over ...      1\n",
       "1  nicaea was convoked by the emperor constantine...      1\n",
       "2  it was rather unfortunate that he vehemently o...      1\n",
       "3  dennis the menace is an american animated seri...      1\n",
       "4  today, on large farms, motorcycles, dogs or me...      1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181468</th>\n",
       "      <td>arguably the most notable was dale earnhardt, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181469</th>\n",
       "      <td>because of the genetic prepotency of the ancie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181470</th>\n",
       "      <td>influenced by serge gainsbourg, the velvet und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181471</th>\n",
       "      <td>northern indiana, for example, contains the in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181472</th>\n",
       "      <td>mythology is alive and well in the modern age ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "181468  arguably the most notable was dale earnhardt, ...      0\n",
       "181469  because of the genetic prepotency of the ancie...      0\n",
       "181470  influenced by serge gainsbourg, the velvet und...      0\n",
       "181471  northern indiana, for example, contains the in...      0\n",
       "181472  mythology is alive and well in the modern age ...      0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.sample(random_state=42, frac=1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130694</th>\n",
       "      <td>on 24 january 1953 mau mau, possibly former se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160318</th>\n",
       "      <td>support for windows media drm (wmdrm) (incompa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98343</th>\n",
       "      <td>when the kingdom became independent, it was co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35321</th>\n",
       "      <td>in may 2007 farley mowat was claimed to be hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136178</th>\n",
       "      <td>world war ii left the united kingdom with an a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143303</th>\n",
       "      <td>fifteen of the sixteen german states, positing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30326</th>\n",
       "      <td>the powerbook 500 series was the mainstay of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58750</th>\n",
       "      <td>this model has been rejected by the scientific...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75834</th>\n",
       "      <td>on 29 august 2010, bogdan made his premier lea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153634</th>\n",
       "      <td>two nations emerged in what is now argentina:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "130694  on 24 january 1953 mau mau, possibly former se...      0\n",
       "160318  support for windows media drm (wmdrm) (incompa...      0\n",
       "98343   when the kingdom became independent, it was co...      0\n",
       "35321   in may 2007 farley mowat was claimed to be hea...      0\n",
       "136178  world war ii left the united kingdom with an a...      0\n",
       "...                                                   ...    ...\n",
       "143303  fifteen of the sixteen german states, positing...      1\n",
       "30326   the powerbook 500 series was the mainstay of t...      0\n",
       "58750   this model has been rejected by the scientific...      0\n",
       "75834   on 29 august 2010, bogdan made his premier lea...      1\n",
       "153634      two nations emerged in what is now argentina:      1\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = full.iloc[0:308504]\n",
    "full_dev = full.iloc[308504:326651]\n",
    "full_test = full.iloc[326651:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326651.4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full) * 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train.to_csv('processed_data/full_train.csv', index=False, header=False)\n",
    "full_dev.to_csv('processed_data/full_dev.csv', index=False, header=False)\n",
    "full_test.to_csv('processed_data/full_test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.6789928169488888\n",
      "Val set accuracy: 0.5785529288587645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "\n",
    "tfidf = Tfidf()\n",
    "X_train = tfidf.fit_transform(full_train.text)\n",
    "y_train = full_train.label\n",
    "\n",
    "X_dev = tfidf.transform(full_dev.text)\n",
    "y_dev = full_dev.label\n",
    "\n",
    "### Attempt 2a: TF-IDF Logistic Regression\n",
    "\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "lr.fit(X_train, y_train)\n",
    "#This one finishes in way fewer iterations! Interesting\n",
    "\n",
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.706196680482501\n",
      "Val set accuracy: 0.6842857142857143\n"
     ]
    }
   ],
   "source": [
    "X_train = tfidf.fit_transform(train.text)\n",
    "y_train = train.label\n",
    "\n",
    "X_dev = tfidf.transform(dev.text)\n",
    "y_dev = dev.label\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set accuracy: {}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Val set accuracy: {}\".format(lr.score(X_dev, y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_full = tfidf.transform(full_dev.text)\n",
    "y_dev_full = full_dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6361933101890119"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_dev_full, y_dev_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', Tfidf()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1,2), (1,3)],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.3s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.3s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.5s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  15.9s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  15.8s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.0s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.0s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.0s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.0s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.1s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.2s\n",
      "[CV] clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .....\n",
      "[CV]  clf__C=0.001, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.5s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.5s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.8s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.5s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.4s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.2s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.4s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.4s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.5s\n",
      "[CV] clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ......\n",
      "[CV]  clf__C=0.01, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  16.3s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.4s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.8s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=   8.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  17.2s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  17.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  17.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.9s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  16.9s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   3.4s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   3.2s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   3.1s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  10.0s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  10.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=   8.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  17.2s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  20.4s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  22.2s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  17.0s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  17.2s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   3.0s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   2.8s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   3.0s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  10.5s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  10.8s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  10.4s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=   8.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  17.1s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  19.7s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  21.6s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  17.1s\n",
      "[CV] clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .......\n",
      "[CV]  clf__C=0.1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  17.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   3.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   4.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   3.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   3.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   3.5s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  13.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  13.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  14.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  11.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  11.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  25.2s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  25.6s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  31.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  19.2s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  19.9s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   4.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   5.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   6.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   6.5s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  20.2s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  15.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  17.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  16.2s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  15.6s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  26.6s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  32.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  36.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  23.2s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  22.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   5.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   3.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   3.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   4.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   4.9s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  17.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  17.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  16.5s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  16.5s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  13.9s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  28.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  33.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  27.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  21.9s\n",
      "[CV] clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total=  28.5s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   9.4s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   9.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=  10.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   7.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 1), total=   8.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  26.6s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  33.4s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  31.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  15.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  34.3s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  35.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total= 1.1min\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  49.3s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  46.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 3), total=  42.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   9.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   9.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   9.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   6.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 1), total=   9.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  36.3s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  33.9s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  32.2s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  37.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  32.9s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  54.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total= 1.2min\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total= 1.2min\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total= 1.1min\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 3), total=  57.6s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   9.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=  10.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   8.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=  10.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 1), total=   9.3s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  38.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  20.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  14.9s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  37.5s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 2), total=  36.3s\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total= 1.3min\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total= 1.1min\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total= 1.1min\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total= 1.0min\n",
      "[CV] clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.75, tfidf__ngram_range=(1, 3), total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 225 out of 225 | elapsed: 57.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_...\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'clf__C': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "search = GridSearchCV(pipeline, parameters, verbose=2)\n",
    "search.fit(train.text, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                   input='content', lowercase=True, max_df=0.25, max_features=None,\n",
       "                   min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                   smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                   sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, use_idf=True, vocabulary=None)),\n",
       "  ('clf',\n",
       "   LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                      warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=0.25, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'clf': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 0.25,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'clf__C': 10,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__dual': False,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__l1_ratio': None,\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__multi_class': 'auto',\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': None,\n",
       " 'clf__solver': 'lbfgs',\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C = 10, max_iter 100, max-df = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "fine_pipe = Pipeline([\n",
    "    ('tfidf', Tfidf()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "fine_parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.375, 0.5),\n",
    "    'tfidf__ngram_range': [(1,2)],\n",
    "    'clf__C': [1, 10, 25, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  15.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   15.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  14.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  14.9s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  11.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  11.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  14.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  13.4s\n",
      "[CV] clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  13.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  11.3s\n",
      "[CV] clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  14.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  23.8s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  13.1s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  17.7s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  17.0s\n",
      "[CV] clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) ..........\n",
      "[CV]  clf__C=1, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  16.6s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  31.8s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  36.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  34.1s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  15.6s\n",
      "[CV] clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  35.5s\n",
      "[CV] clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  34.4s\n",
      "[CV] clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  32.2s\n",
      "[CV] clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  32.8s\n",
      "[CV] clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  28.7s\n",
      "[CV] clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=10, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  31.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  36.6s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  36.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  33.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  46.0s\n",
      "[CV] clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=10, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  34.9s\n",
      "[CV] clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  47.9s\n",
      "[CV] clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  24.6s\n",
      "[CV] clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  44.7s\n",
      "[CV] clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  41.1s\n",
      "[CV] clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  42.4s\n",
      "[CV] clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  43.0s\n",
      "[CV] clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  51.3s\n",
      "[CV] clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  48.5s\n",
      "[CV] clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  33.1s\n",
      "[CV] clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=25, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  51.3s\n",
      "[CV] clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  56.7s\n",
      "[CV] clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  45.8s\n",
      "[CV] clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  45.8s\n",
      "[CV] clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2) .........\n",
      "[CV]  clf__C=25, tfidf__max_df=0.5, tfidf__ngram_range=(1, 2), total=  45.2s\n",
      "[CV] clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  59.8s\n",
      "[CV] clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  59.4s\n",
      "[CV] clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total=  53.7s\n",
      "[CV] clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2) ........\n",
      "[CV]  clf__C=50, tfidf__max_df=0.25, tfidf__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__C=50, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=50, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total=  52.0s\n",
      "[CV] clf__C=50, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n",
      "[CV]  clf__C=50, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2), total= 1.1min\n",
      "[CV] clf__C=50, tfidf__max_df=0.375, tfidf__ngram_range=(1, 2) .......\n"
     ]
    }
   ],
   "source": [
    "fine_search = GridSearchCV(fine_pipe, fine_parameters, verbose=2)\n",
    "fine_search.fit(train.text, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
