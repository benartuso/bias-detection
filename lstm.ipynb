{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J9gMGtyGgv6",
        "outputId": "97c6757c-471a-480b-a8eb-dbc6cae9da25"
      },
      "source": [
        "# Necessary packages\n",
        "\n",
        "import sys, os, random, math, sys\n",
        "import torch, spacy\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm \n",
        "\n",
        "## Random seeds, to make the results reproducible\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(torch.randn(5))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVnZi2hzG3CJ",
        "outputId": "04746533-672a-4b3a-f002-1b81ad6daee5"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns7AvX1zHsYJ",
        "outputId": "a413b831-ad56-45f1-f561-8f7e64209197"
      },
      "source": [
        "#Read in data...\n",
        "\n",
        "!rm -rf processed_data processed_data.zip\n",
        "import urllib.request\n",
        "url = \"https://github.com/benartuso/bias-detection/blob/main/data/processed_data.zip?raw=true\"\n",
        "filename, headers = urllib.request.urlretrieve(url, filename=\"processed_data.zip\")\n",
        "!unzip processed_data.zip\n",
        "print(\"Done\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  processed_data.zip\n",
            "   creating: processed_data/\n",
            "  inflating: processed_data/dev_lstm.csv  \n",
            "  inflating: processed_data/biased.word.dev  \n",
            "  inflating: processed_data/dev_ann.csv  \n",
            "  inflating: processed_data/train_ann.csv  \n",
            "  inflating: processed_data/biased.word.test  \n",
            "  inflating: processed_data/test_ann.csv  \n",
            "  inflating: processed_data/biased.word.train  \n",
            "  inflating: processed_data/train_lstm.csv  \n",
            "  inflating: processed_data/test_lstm.csv  \n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maicpgscG5HD"
      },
      "source": [
        "#This code is adapted from neural_classifier() lecture example code.\n",
        "from torchtext.data import Field, ReversibleField, Dataset, TabularDataset, BucketIterator, Iterator\n",
        "spacy_en = spacy.load('en')\n",
        "def tokenize_fn(text):\n",
        "    \"\"\" Tokenization function - split apart on spaces.\n",
        "        This is sufficient for tokenization, since puctuation etc. \n",
        "        have already been handled by the prof's preprocessing of wiki2\n",
        "    \"\"\"\n",
        "    # return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "    return text.strip().split()\n",
        "\n",
        "\n",
        "def reader(suffix=\".tsv\", rpath=\"sst\", batch_size=8, min_freq=2):\n",
        "    \"\"\"\n",
        "    - suffix: data file suffix\n",
        "    - rpath: path to the data files\n",
        "    - batch_size: mini-batch size\n",
        "    - min_freq: word frequency cutoff, frequency less than min_freq will be removed when building the vocab\n",
        "    \"\"\"\n",
        "    # Utterance Field: text\n",
        "    TXT = Field(sequential=True, tokenize=tokenize_fn, init_token=None, eos_token=None, lower=True)\n",
        "    LABEL = Field(sequential=False, unk_token=None, dtype=torch.long, use_vocab=False)\n",
        "    #Treat label as a sequential field, pad it as well!\n",
        "\n",
        "\n",
        "    # Create a Dataset instance\n",
        "    fields = [(\"text\", TXT), (\"label\", LABEL)]\n",
        "    trn_data = TabularDataset(os.path.join(rpath,'train_lstm'+suffix), format=\"CSV\", fields=fields, skip_header=True)\n",
        "    val_data = TabularDataset(os.path.join(rpath,'dev_lstm'+suffix), format=\"CSV\", fields=fields, skip_header=True)\n",
        "    tst_data = TabularDataset(os.path.join(rpath, 'test_lstm'+suffix), format=\"CSV\", fields=fields, skip_header=True)\n",
        "    #No test data\n",
        "\n",
        "    \n",
        "    # Split\n",
        "    # Build vocab using training data\n",
        "    TXT.build_vocab(trn_data, min_freq=min_freq) # or max_size=10000\n",
        "    LABEL.build_vocab(trn_data, min_freq=min_freq)\n",
        "    # \n",
        "    train_iter, val_iter, test_iter = BucketIterator.splits((trn_data, val_data, tst_data), # data\n",
        "                                                             batch_size=batch_size, # \n",
        "                                                             sort=True, # sort_key not specified\n",
        "                                                             sort_key = lambda x : len(x.text),\n",
        "                                                             shuffle=False, # shuffle between epochs\n",
        "                                                             repeat=False)\n",
        "    return train_iter, val_iter, test_iter, TXT"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOU8-f0kHSRo"
      },
      "source": [
        "train_iter, val_iter, test_iter, txtfield = reader(suffix='.csv', rpath=\"processed_data\", min_freq=5)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn2-ZYrCH8wd",
        "outputId": "78077c4a-b3f7-43e0-8992-19877415648a"
      },
      "source": [
        "vocab_size = len(txtfield.vocab)\n",
        "print(\"Vocab size = {}\".format(vocab_size))\n",
        "pad = txtfield.vocab.stoi[txtfield.pad_token]\n",
        "\n",
        "print(\"[TRAIN]:%d (dataset:%d)\\t[VAL]:%d (dataset:%d)\\t\"\n",
        "    % (len(train_iter), len(train_iter.dataset),\n",
        "    len(val_iter), len(val_iter.dataset)))\n",
        "print(\"[vocab]:%d\" % (vocab_size))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size = 13321\n",
            "[TRAIN]:13451 (dataset:107606)\t[VAL]:175 (dataset:1400)\t\n",
            "[vocab]:13321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgCckmkUIHRl"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, pad_token, n_input=32, n_hidden=32, n_layers=1, drop_prob=0, lr=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.n_input = n_input\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "    self.pad = pad_token\n",
        "    self.emb_layer = nn.Embedding(vocab_size, n_input, padding_idx = self.pad)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.lstm = nn.LSTM(n_input, n_hidden, n_layers, dropout=drop_prob, batch_first=False)\n",
        "    self.fc = nn.Linear(n_hidden, 2)\n",
        "    self.lr = lr\n",
        "  def forward(self, batch, hidden):\n",
        "    input, label = batch.text.cuda(), batch.label.cuda()\n",
        "\n",
        "    #Create word embeddings\n",
        "    embedded = self.emb_layer(input.long())\n",
        "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "    out = self.dropout(lstm_output)\n",
        "\n",
        "    out = self.fc(out)\n",
        "    out = out[-1]\n",
        "    out = F.log_softmax(out, dim=-1) #Is this right? \n",
        "    #label = label.permute(1,0)\n",
        "\n",
        "    #out = out.permute(1, 2, 0)\n",
        "    loss = F.cross_entropy(out, label, reduction=\"mean\", ignore_index=self.pad)\n",
        "\n",
        "    return loss, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(), \n",
        "                 weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        \n",
        "    return hidden"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1YrA2g0SiEU"
      },
      "source": [
        "def batch_train(batch, hidden, model, optimizer):\n",
        "\n",
        "  hidden = model.init_hidden(batch_size=batch.text.shape[1])\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss, hidden = model(batch, hidden)\n",
        "  loss.backward()\n",
        "  clip_grad_norm(model.parameters(), grad_clip)\n",
        "  optimizer.step()\n",
        "\n",
        "  return model, loss.item(), hidden"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "501-10a9Svz1",
        "outputId": "1046cf42-7938-4eee-e3b5-919dda59a8c6"
      },
      "source": [
        "#Initialize model with params specified in HW description\n",
        "model = LSTM(vocab_size, pad, n_input=32, n_hidden=32, n_layers=1, drop_prob=0.9, lr=0.1)\n",
        "#Move to colab GPU for speed updates\n",
        "model.cuda()\n",
        "#Initialize hidden state\n",
        "hidden = model.init_hidden(batch_size=8)\n",
        "#Initialize optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0)\n",
        "# the norm of grad clipping\n",
        "grad_clip = 1.0\n",
        "\n",
        "# ------------------------------------\n",
        "# 3. Define the numbers of training epochs and validation steps (val_steps not used here)\n",
        "epoch, val_step = 5, 50\n",
        "\n",
        "# ------------------------------------\n",
        "# 4. Training iterations\n",
        "TrnLoss, ValLoss, ValAcc = [], [], []\n",
        "total_batch = 0\n",
        "for e in range(epoch):\n",
        "    epoch_loss = []\n",
        "    print(\"Beginning epoch {}\".format(e+1))\n",
        "    for b, batch in (enumerate(train_iter)):\n",
        "        total_batch += 1\n",
        "        # Update parameters with one batch\n",
        "        model, loss, hidden = batch_train(batch, hidden, model, optimizer)\n",
        "        epoch_loss.append(loss)\n",
        "        if b % 500 == 0:\n",
        "          print(loss)\n",
        "    print(\"Epoch {} average loss: {}\".format((e+1), np.mean(epoch_loss)))    "
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning epoch 1\n",
            "0.7209597826004028\n",
            "0.008460501208901405\n",
            "0.0\n",
            "0.0072160386480391026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-50e1830898ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Update parameters with one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-3579df9ea608>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(batch, hidden, model, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC9RbbEUS1wK"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    }
  ]
}